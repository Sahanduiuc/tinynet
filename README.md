TinyNet is a tiny DL framework designed in a modular, Keras-like, fashion. The aim is to provide understanding of basic building blocks of neural networks and their coordination.

Features:
- Batch, mini-batch and stochastic GD
- Layers:
  - Dense
  - Convolutional (2D)
  - Pooling (2D)
- Regularizers:
  - L2
  - Dropout
- Optimizers:
  - Momentum
  - Adam
- Batch normalization
- Gradient Checking

Notebooks:

[Example with sklearn dataset](https://nbviewer.jupyter.org/github/polakowo/tinynet/blob/master/examples/sklearn.ipynb)

![](examples/images/sklearn.png)

[Example with MNIST dataset](https://nbviewer.jupyter.org/github/polakowo/tinynet/blob/master/examples/mnist.ipynb)

![](examples/images/mnist.png)
